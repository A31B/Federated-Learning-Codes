{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rPpL2APKcynz",
        "outputId": "70dd4922-8d34-46d9-cdfa-a8f945942f94"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results saved to 'model_results_global.csv'\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "# Load the dataset into a pandas dataframe\n",
        "df = pd.read_csv('ADSXLIST_07Sep2023.csv')\n",
        "\n",
        "common_features = ['AXRASH', 'AXMUSCLE', 'AXURNFRQ', 'AXENERGY', 'AXDROWSY', 'AXDIZZY', 'AXBREATH', 'AXCOUGH']\n",
        "\n",
        "# Define categorical features for one-hot encoding\n",
        "categorical_features = ['VISCODE', 'VISCODE2', 'SITEID']\n",
        "df = pd.get_dummies(df, columns=categorical_features)\n",
        "\n",
        "# Exclude any non-numeric columns (e.g., dates or other string columns)\n",
        "numeric_columns = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "\n",
        "# Ensure 'Phase' (target variable) is not in the features list\n",
        "if 'Phase' in numeric_columns:\n",
        "    numeric_columns.remove('Phase')\n",
        "\n",
        "# Define the number of iterations and splits\n",
        "num_iterations = 10\n",
        "num_splits = 6\n",
        "\n",
        "# Lists to store global accuracy and precision for each iteration\n",
        "global_accuracy_all_iterations = []\n",
        "global_precision_all_iterations = []\n",
        "\n",
        "for iteration in range(num_iterations):\n",
        "\n",
        "    iteration_global_accuracy = [0] * num_splits\n",
        "    iteration_global_precision = [0] * num_splits\n",
        "\n",
        "    # Shuffle the data randomly for each iteration\n",
        "    df_shuffled = df.sample(frac=1, random_state=iteration * 123)\n",
        "\n",
        "    # Initial Splitting of the data into sections\n",
        "    split_size = len(df_shuffled) // num_splits\n",
        "    data_splits = [df_shuffled.iloc[i * split_size: (i + 1) * split_size] for i in range(num_splits)]\n",
        "\n",
        "    # Data Shifting: Move 25% data from each node to the next\n",
        "    for i in range(num_splits):\n",
        "        next_index = (i + 1) % num_splits\n",
        "        data_to_shift = data_splits[i].sample(frac=0.25, random_state=iteration)\n",
        "        data_splits[i] = data_splits[i].drop(data_to_shift.index)\n",
        "        data_splits[next_index] = pd.concat([data_splits[next_index], data_to_shift])\n",
        "\n",
        "    # Lists for storing model parameters\n",
        "    coefficients_list = []\n",
        "    intercepts_list = []\n",
        "\n",
        "    # Train local models and collect their parameters\n",
        "    for i in range(num_splits):\n",
        "        split_data = data_splits[i]\n",
        "        # Prepare the data for each node\n",
        "        if i == 0:  # Node 1\n",
        "            split_data = split_data[split_data['AXRASH'] == 2]\n",
        "            features = ['AXRASH']\n",
        "        elif i == 1:  # Node 2\n",
        "            split_data = split_data[split_data['AXCOUGH'] == 1]\n",
        "            features = ['AXCOUGH']\n",
        "        else:  # Nodes 3 to 6\n",
        "            features = common_features\n",
        "\n",
        "        X = split_data[numeric_columns].copy()\n",
        "        y = split_data['Phase'].copy()\n",
        "\n",
        "        # Data preprocessing steps\n",
        "        X.fillna(0, inplace=True)\n",
        "\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=359)\n",
        "\n",
        "        model = LogisticRegression(max_iter=1000)\n",
        "        model.fit(X_train, y_train)\n",
        "\n",
        "        coefficients_list.append(model.coef_)\n",
        "        intercepts_list.append(model.intercept_)\n",
        "\n",
        "        y_pred = model.predict(X_test)\n",
        "        local_accuracy = accuracy_score(y_test, y_pred)\n",
        "        local_precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
        "\n",
        "    # Federated Averaging: Calculate mean of coefficients and intercepts\n",
        "    avg_coefficients = np.mean(coefficients_list, axis=0)\n",
        "    avg_intercepts = np.mean(intercepts_list, axis=0)\n",
        "\n",
        "    global_model = LogisticRegression(max_iter=1000)\n",
        "    global_model.coef_ = avg_coefficients\n",
        "    global_model.intercept_ = avg_intercepts\n",
        "\n",
        "    # Fit the global model on a small but representative subset of data to initialize 'classes_'\n",
        "    subset = df_shuffled.drop_duplicates(subset='Phase').head(10)\n",
        "    subset_X = subset[numeric_columns].copy()\n",
        "    subset_X.fillna(0, inplace=True)\n",
        "    subset_y = subset['Phase'].copy()\n",
        "    global_model.fit(subset_X, subset_y)\n",
        "\n",
        "    # Sending global model back to nodes 3 to 6 for testing\n",
        "    nodes_to_test = [2, 3, 4, 5]  # Python uses 0-indexing, so node 3 is index 2, etc.\n",
        "    for node_index in nodes_to_test:\n",
        "        split_data = data_splits[node_index]\n",
        "        X_test = split_data[numeric_columns].copy()\n",
        "        y_test = split_data['Phase'].copy()\n",
        "\n",
        "        # Data preprocessing (if necessary)\n",
        "        X_test.fillna(0, inplace=True)\n",
        "\n",
        "        y_pred = global_model.predict(X_test)\n",
        "        node_accuracy = accuracy_score(y_test, y_pred)\n",
        "        node_precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
        "\n",
        "        # Store the results for each node\n",
        "        iteration_global_accuracy[node_index] = node_accuracy\n",
        "        iteration_global_precision[node_index] = node_precision\n",
        "    # Store global accuracy and precision for this iteration\n",
        "    global_accuracy_all_iterations.append(iteration_global_accuracy)\n",
        "    global_precision_all_iterations.append(iteration_global_precision)\n",
        "\n",
        "# Organize the results into a DataFrame\n",
        "results_df = pd.DataFrame({\n",
        "    'Global Accuracy': global_accuracy_all_iterations,\n",
        "    'Global Precision': global_precision_all_iterations\n",
        "})\n",
        "\n",
        "# Save the results to a CSV file\n",
        "results_df.to_csv('model_results_global.csv', index=False)\n",
        "\n",
        "print(\"Results saved to 'model_results_global.csv'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "# Load the dataset into a pandas dataframe\n",
        "df = pd.read_csv('ADSXLIST_07Sep2023.csv')\n",
        "\n",
        "# Common features\n",
        "common_features = ['AXRASH', 'AXMUSCLE', 'AXURNFRQ', 'AXENERGY', 'AXDROWSY', 'AXDIZZY', 'AXBREATH', 'AXCOUGH']\n",
        "\n",
        "# Define categorical features for one-hot encoding\n",
        "categorical_features = ['VISCODE', 'VISCODE2', 'SITEID']\n",
        "df = pd.get_dummies(df, columns=categorical_features)\n",
        "\n",
        "# Exclude any non-numeric columns (e.g., dates or other string columns)\n",
        "numeric_columns = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "\n",
        "# Ensure 'Phase' (target variable) is not in the features list\n",
        "if 'Phase' in numeric_columns:\n",
        "    numeric_columns.remove('Phase')\n",
        "\n",
        "# Define the number of iterations and splits\n",
        "num_iterations = 10\n",
        "num_splits = 6\n",
        "\n",
        "# Lists to store global accuracy and precision for each iteration\n",
        "global_accuracy_all_iterations = []\n",
        "global_precision_all_iterations = []\n",
        "\n",
        "for iteration in range(num_iterations):\n",
        "\n",
        "    iteration_global_accuracy = [0] * num_splits\n",
        "    iteration_global_precision = [0] * num_splits\n",
        "\n",
        "    # Shuffle the data randomly for each iteration\n",
        "    df_shuffled = df.sample(frac=1, random_state=iteration * 123)\n",
        "\n",
        "    # Initial Splitting of the data into sections\n",
        "    split_size = len(df_shuffled) // num_splits\n",
        "    data_splits = [df_shuffled.iloc[i * split_size: (i + 1) * split_size] for i in range(num_splits)]\n",
        "\n",
        "    # Data Shifting: Move 25% data from each node to the next\n",
        "    for i in range(num_splits):\n",
        "        next_index = (i + 1) % num_splits\n",
        "        data_to_shift = data_splits[i].sample(frac=0.25, random_state=iteration)\n",
        "        data_splits[i] = data_splits[i].drop(data_to_shift.index)\n",
        "        data_splits[next_index] = pd.concat([data_splits[next_index], data_to_shift])\n",
        "\n",
        "    # Lists for storing model parameters\n",
        "    coefficients_list = []\n",
        "    intercepts_list = []\n",
        "\n",
        "    # Train local models and collect their parameters (only for nodes 3 to 6)\n",
        "    for i in range(2, num_splits):  # Start from index 2 (Node 3) to num_splits (Node 6)\n",
        "        split_data = data_splits[i]\n",
        "\n",
        "        X = split_data[numeric_columns].copy()\n",
        "        y = split_data['Phase'].copy()\n",
        "\n",
        "        # Data preprocessing steps\n",
        "        X.fillna(0, inplace=True)\n",
        "\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=359)\n",
        "\n",
        "        model = LogisticRegression(max_iter=1000)\n",
        "        model.fit(X_train, y_train)\n",
        "\n",
        "        coefficients_list.append(model.coef_)\n",
        "        intercepts_list.append(model.intercept_)\n",
        "\n",
        "    # Federated Averaging: Calculate mean of coefficients and intercepts (only from nodes 3 to 6)\n",
        "    avg_coefficients = np.mean(coefficients_list, axis=0)\n",
        "    avg_intercepts = np.mean(intercepts_list, axis=0)\n",
        "\n",
        "    global_model = LogisticRegression(max_iter=1000)\n",
        "    global_model.coef_ = avg_coefficients\n",
        "    global_model.intercept_ = avg_intercepts\n",
        "\n",
        "    # Fit the global model on a small but representative subset of data to initialize 'classes_'\n",
        "    subset = df_shuffled[df_shuffled.index.isin(data_splits[2].index | data_splits[3].index | data_splits[4].index | data_splits[5].index)].drop_duplicates(subset='Phase').head(10)\n",
        "    subset_X = subset[numeric_columns].copy()\n",
        "    subset_X.fillna(0, inplace=True)\n",
        "    subset_y = subset['Phase'].copy()\n",
        "    global_model.fit(subset_X, subset_y)\n",
        "\n",
        "    # Testing the global model on nodes 3 to 6\n",
        "    for node_index in range(2, num_splits):  # Nodes 3 to 6\n",
        "        split_data = data_splits[node_index]\n",
        "        X_test = split_data[numeric_columns].copy()\n",
        "        y_test = split_data['Phase'].copy()\n",
        "\n",
        "        # Data preprocessing\n",
        "        X_test.fillna(0, inplace=True)\n",
        "\n",
        "        y_pred = global_model.predict(X_test)\n",
        "        node_accuracy = accuracy_score(y_test, y_pred)\n",
        "        node_precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
        "\n",
        "        # Store results for each node\n",
        "        iteration_global_accuracy[node_index] = node_accuracy\n",
        "        iteration_global_precision[node_index] = node_precision\n",
        "\n",
        "    # Store global accuracy and precision for this iteration\n",
        "    global_accuracy_all_iterations.append(iteration_global_accuracy)\n",
        "    global_precision_all_iterations.append(iteration_global_precision)\n",
        "\n",
        "# Organize the results into a DataFrame\n",
        "results_df = pd.DataFrame({\n",
        "    'Global Accuracy': global_accuracy_all_iterations,\n",
        "    'Global Precision': global_precision_all_iterations\n",
        "})\n",
        "\n",
        "# Save the results to a CSV file\n",
        "results_df.to_csv('model_results_global.csv', index=False)\n",
        "\n",
        "print(\"Results saved to 'model_results_global.csv'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wb6Vk6CDmzwV",
        "outputId": "4240c5b6-ce29-4945-b825-21383391344a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-46867f982169>:82: FutureWarning: Index.__or__ operating as a set operation is deprecated, in the future this will be a logical operation matching Series.__or__.  Use index.union(other) instead.\n",
            "  subset = df_shuffled[df_shuffled.index.isin(data_splits[2].index | data_splits[3].index | data_splits[4].index | data_splits[5].index)].drop_duplicates(subset='Phase').head(10)\n",
            "<ipython-input-3-46867f982169>:82: FutureWarning: Index.__or__ operating as a set operation is deprecated, in the future this will be a logical operation matching Series.__or__.  Use index.union(other) instead.\n",
            "  subset = df_shuffled[df_shuffled.index.isin(data_splits[2].index | data_splits[3].index | data_splits[4].index | data_splits[5].index)].drop_duplicates(subset='Phase').head(10)\n",
            "<ipython-input-3-46867f982169>:82: FutureWarning: Index.__or__ operating as a set operation is deprecated, in the future this will be a logical operation matching Series.__or__.  Use index.union(other) instead.\n",
            "  subset = df_shuffled[df_shuffled.index.isin(data_splits[2].index | data_splits[3].index | data_splits[4].index | data_splits[5].index)].drop_duplicates(subset='Phase').head(10)\n",
            "<ipython-input-3-46867f982169>:82: FutureWarning: Index.__or__ operating as a set operation is deprecated, in the future this will be a logical operation matching Series.__or__.  Use index.union(other) instead.\n",
            "  subset = df_shuffled[df_shuffled.index.isin(data_splits[2].index | data_splits[3].index | data_splits[4].index | data_splits[5].index)].drop_duplicates(subset='Phase').head(10)\n",
            "<ipython-input-3-46867f982169>:82: FutureWarning: Index.__or__ operating as a set operation is deprecated, in the future this will be a logical operation matching Series.__or__.  Use index.union(other) instead.\n",
            "  subset = df_shuffled[df_shuffled.index.isin(data_splits[2].index | data_splits[3].index | data_splits[4].index | data_splits[5].index)].drop_duplicates(subset='Phase').head(10)\n",
            "<ipython-input-3-46867f982169>:82: FutureWarning: Index.__or__ operating as a set operation is deprecated, in the future this will be a logical operation matching Series.__or__.  Use index.union(other) instead.\n",
            "  subset = df_shuffled[df_shuffled.index.isin(data_splits[2].index | data_splits[3].index | data_splits[4].index | data_splits[5].index)].drop_duplicates(subset='Phase').head(10)\n",
            "<ipython-input-3-46867f982169>:82: FutureWarning: Index.__or__ operating as a set operation is deprecated, in the future this will be a logical operation matching Series.__or__.  Use index.union(other) instead.\n",
            "  subset = df_shuffled[df_shuffled.index.isin(data_splits[2].index | data_splits[3].index | data_splits[4].index | data_splits[5].index)].drop_duplicates(subset='Phase').head(10)\n",
            "<ipython-input-3-46867f982169>:82: FutureWarning: Index.__or__ operating as a set operation is deprecated, in the future this will be a logical operation matching Series.__or__.  Use index.union(other) instead.\n",
            "  subset = df_shuffled[df_shuffled.index.isin(data_splits[2].index | data_splits[3].index | data_splits[4].index | data_splits[5].index)].drop_duplicates(subset='Phase').head(10)\n",
            "<ipython-input-3-46867f982169>:82: FutureWarning: Index.__or__ operating as a set operation is deprecated, in the future this will be a logical operation matching Series.__or__.  Use index.union(other) instead.\n",
            "  subset = df_shuffled[df_shuffled.index.isin(data_splits[2].index | data_splits[3].index | data_splits[4].index | data_splits[5].index)].drop_duplicates(subset='Phase').head(10)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results saved to 'model_results_global.csv'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-46867f982169>:82: FutureWarning: Index.__or__ operating as a set operation is deprecated, in the future this will be a logical operation matching Series.__or__.  Use index.union(other) instead.\n",
            "  subset = df_shuffled[df_shuffled.index.isin(data_splits[2].index | data_splits[3].index | data_splits[4].index | data_splits[5].index)].drop_duplicates(subset='Phase').head(10)\n"
          ]
        }
      ]
    }
  ]
}